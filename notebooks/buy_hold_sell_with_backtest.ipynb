{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9241ef-2731-4422-8dc1-b7d05501e72b",
      "metadata": {
        "id": "ed9241ef-2731-4422-8dc1-b7d05501e72b",
        "outputId": "e5c205c9-c9e3-47f8-c613-3f612d6372af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAGR: 55.29%\n",
            "Sharpe Ratio: 1.18\n",
            "Sortino Ratio: 1.68\n",
            "Max Drawdown: -48.64%\n",
            "ðŸ“¤ Uploaded to Blob: backtest_outputs/backtest_results.csv\n",
            "ðŸ“¤ Uploaded to Blob: backtest_outputs/trade_signals.csv\n",
            "ðŸ“¤ Uploaded to Blob: backtest_outputs/backtest_summary.csv\n",
            "âœ… Backtest complete & uploaded to Blob.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "from azure.storage.blob import ContainerClient, BlobClient\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()  # loads .env file if present (for local dev)\n",
        "\n",
        "AZURE_CONN_STR = os.getenv(\"AZURE_CONN_STR\")\n",
        "CONTAINER = \"stock-data\"\n",
        "MODEL_READY_FOLDER = \"model_ready_data\"\n",
        "PREDICTIONS_FOLDER = \"predictions\"\n",
        "OUTPUT_FOLDER = \"backtest_outputs\"\n",
        "INITIAL_CAPITAL = 100000\n",
        "TOP_N = 10\n",
        "REBALANCE_FREQ = \"Q\"  # Q = Quarterly, M = Monthly, W = Weekly\n",
        "\n",
        "# =========================\n",
        "# METRICS\n",
        "# =========================\n",
        "def calculate_cagr(df):\n",
        "    start_val = df[\"portfolio_value\"].iloc[0]\n",
        "    end_val = df[\"portfolio_value\"].iloc[-1]\n",
        "    years = (df[\"date\"].iloc[-1] - df[\"date\"].iloc[0]).days / 365.25\n",
        "    return (end_val / start_val) ** (1 / years) - 1 if years > 0 else np.nan\n",
        "\n",
        "def calculate_sharpe(df, rf_rate=0.02):\n",
        "    returns = df[\"portfolio_value\"].pct_change().dropna()\n",
        "    excess = returns - rf_rate / 252\n",
        "    return np.sqrt(252) * excess.mean() / excess.std() if excess.std() != 0 else np.nan\n",
        "\n",
        "def calculate_sortino(df, rf_rate=0.02):\n",
        "    returns = df[\"portfolio_value\"].pct_change().dropna()\n",
        "    downside = returns[returns < 0]\n",
        "    excess = returns - rf_rate / 252\n",
        "    return np.sqrt(252) * excess.mean() / downside.std() if downside.std() != 0 else np.nan\n",
        "\n",
        "def calculate_mdd(df):\n",
        "    roll_max = df[\"portfolio_value\"].cummax()\n",
        "    drawdown = (df[\"portfolio_value\"] - roll_max) / roll_max\n",
        "    return drawdown.min()\n",
        "\n",
        "# =========================\n",
        "# AZURE BLOB FUNCTIONS\n",
        "# =========================\n",
        "container_client = ContainerClient.from_connection_string(AZURE_CONN_STR, CONTAINER)\n",
        "\n",
        "def read_parquet_from_blob(blob_name):\n",
        "    blob_client = container_client.get_blob_client(blob_name)\n",
        "    data = blob_client.download_blob().readall()\n",
        "    return pd.read_parquet(io.BytesIO(data))\n",
        "\n",
        "def upload_to_blob(local_path, blob_path):\n",
        "    blob_client = container_client.get_blob_client(blob_path)\n",
        "    with open(local_path, \"rb\") as data:\n",
        "        blob_client.upload_blob(data, overwrite=True)\n",
        "    print(f\"ðŸ“¤ Uploaded to Blob: {blob_path}\")\n",
        "\n",
        "# =========================\n",
        "# GET PRICE FILES MAP\n",
        "# =========================\n",
        "all_blobs = list(container_client.list_blobs(name_starts_with=f\"{MODEL_READY_FOLDER}/\"))\n",
        "ticker_files = {os.path.basename(b.name).split(\"_\")[0]: b.name for b in all_blobs if b.name.endswith(\".parquet\")}\n",
        "\n",
        "# =========================\n",
        "# LOAD RANKING DATA\n",
        "# =========================\n",
        "val_df = read_parquet_from_blob(f\"{PREDICTIONS_FOLDER}/final_scored_val.parquet\")\n",
        "test_df = read_parquet_from_blob(f\"{PREDICTIONS_FOLDER}/final_scored_test.parquet\")\n",
        "ranking_df = pd.concat([val_df, test_df], ignore_index=True)\n",
        "\n",
        "ranking_df.rename(columns={\n",
        "    \"Date\": \"date\",\n",
        "    \"Ticker\": \"ticker\",\n",
        "    \"model1_prob_m1\": \"model1_prob\",\n",
        "    \"model2_pred_return_m2\": \"model2_pred_return\"\n",
        "}, inplace=True)\n",
        "\n",
        "ranking_df[\"date\"] = pd.to_datetime(ranking_df[\"date\"])\n",
        "ranking_df.sort_values([\"date\", \"final_score\"], ascending=[True, False], inplace=True)\n",
        "\n",
        "# =========================\n",
        "# BACKTEST\n",
        "# =========================\n",
        "portfolio_value = INITIAL_CAPITAL\n",
        "portfolio_history = []\n",
        "trade_signals = []  # one row per holding per rebalance with action+sector\n",
        "price_cache = {}\n",
        "sector_cache = {}\n",
        "\n",
        "available_dates = sorted(ranking_df[\"date\"].unique())\n",
        "\n",
        "# Determine rebalancing schedule\n",
        "if REBALANCE_FREQ == \"Q\":\n",
        "    rebalance_dates = pd.date_range(start=available_dates[0], end=available_dates[-1], freq=\"QS\")\n",
        "elif REBALANCE_FREQ == \"M\":\n",
        "    rebalance_dates = pd.date_range(start=available_dates[0], end=available_dates[-1], freq=\"MS\")\n",
        "elif REBALANCE_FREQ == \"W\":\n",
        "    rebalance_dates = pd.date_range(start=available_dates[0], end=available_dates[-1], freq=\"W\")\n",
        "else:\n",
        "    rebalance_dates = pd.to_datetime(available_dates)\n",
        "\n",
        "current_holdings = []\n",
        "\n",
        "for i, rebalance_date in enumerate(rebalance_dates):\n",
        "    # Nearest available trading date\n",
        "    day_df = ranking_df[ranking_df[\"date\"] >= rebalance_date]\n",
        "    if day_df.empty:\n",
        "        continue\n",
        "    rebalance_actual_date = day_df[\"date\"].min()\n",
        "\n",
        "    # Pick top N tickers\n",
        "    top_stocks = day_df[day_df[\"date\"] == rebalance_actual_date].sort_values(\"final_score\", ascending=False).head(TOP_N)\n",
        "    new_holdings = top_stocks[\"ticker\"].tolist()\n",
        "\n",
        "    buys = [t for t in new_holdings if t not in current_holdings]\n",
        "    sells = [t for t in current_holdings if t not in new_holdings]\n",
        "    holds = [t for t in new_holdings if t in current_holdings]\n",
        "\n",
        "    # Ensure price+sector info cached\n",
        "    for ticker in set(new_holdings + sells):\n",
        "        if ticker not in ticker_files:\n",
        "            continue\n",
        "        if ticker not in price_cache:\n",
        "            df_price = read_parquet_from_blob(ticker_files[ticker])\n",
        "            df_price.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
        "            df_price[\"date\"] = pd.to_datetime(df_price[\"date\"])\n",
        "            price_cache[ticker] = df_price\n",
        "            sector_cache[ticker] = df_price[\"Sector\"].iloc[0] if \"Sector\" in df_price.columns else \"Unknown\"\n",
        "\n",
        "    # Add trade signal rows\n",
        "    for ticker in buys:\n",
        "        trade_signals.append({\n",
        "            \"rebalance_date\": rebalance_actual_date,\n",
        "            \"ticker\": ticker,\n",
        "            \"sector\": sector_cache.get(ticker, \"Unknown\"),\n",
        "            \"action\": \"BUY\"\n",
        "        })\n",
        "    for ticker in holds:\n",
        "        trade_signals.append({\n",
        "            \"rebalance_date\": rebalance_actual_date,\n",
        "            \"ticker\": ticker,\n",
        "            \"sector\": sector_cache.get(ticker, \"Unknown\"),\n",
        "            \"action\": \"HOLD\"\n",
        "        })\n",
        "    for ticker in sells:\n",
        "        trade_signals.append({\n",
        "            \"rebalance_date\": rebalance_actual_date,\n",
        "            \"ticker\": ticker,\n",
        "            \"sector\": sector_cache.get(ticker, \"Unknown\"),\n",
        "            \"action\": \"SELL\"\n",
        "        })\n",
        "\n",
        "    current_holdings = new_holdings\n",
        "    equal_allocation = portfolio_value / len(current_holdings) if current_holdings else 0\n",
        "\n",
        "    # Advance until next rebalance\n",
        "    next_rebalance_date = (rebalance_dates[i + 1]\n",
        "                           if i + 1 < len(rebalance_dates)\n",
        "                           else available_dates[-1])\n",
        "\n",
        "    current_date = rebalance_actual_date\n",
        "    while current_date <= next_rebalance_date:\n",
        "        day_returns = []\n",
        "        for ticker in current_holdings:\n",
        "            df_price = price_cache.get(ticker)\n",
        "            if df_price is None:\n",
        "                continue\n",
        "\n",
        "            price_col = next((c for c in [\"Adj Close\", \"adj_close\", \"Close\", \"close\"] if c in df_price.columns), None)\n",
        "            if price_col is None:\n",
        "                continue\n",
        "\n",
        "            today_idx = df_price[\"date\"].searchsorted(current_date)\n",
        "            next_idx = today_idx + 1 if today_idx + 1 < len(df_price) else None\n",
        "            if next_idx is None:\n",
        "                continue\n",
        "\n",
        "            today_price = df_price.iloc[today_idx][price_col]\n",
        "            next_price = df_price.iloc[next_idx][price_col]\n",
        "            ret = (next_price - today_price) / today_price\n",
        "            day_returns.append(ret)\n",
        "\n",
        "        if day_returns:\n",
        "            portfolio_value *= (1 + np.mean(day_returns))\n",
        "\n",
        "        portfolio_history.append({\"date\": current_date, \"portfolio_value\": portfolio_value})\n",
        "        current_date += pd.Timedelta(days=1)\n",
        "\n",
        "# =========================\n",
        "# RESULTS\n",
        "# =========================\n",
        "results_df = pd.DataFrame(portfolio_history).drop_duplicates(subset=\"date\").sort_values(\"date\")\n",
        "if results_df.empty:\n",
        "    raise ValueError(\"Portfolio history is empty â€” check date alignment.\")\n",
        "\n",
        "cagr = calculate_cagr(results_df)\n",
        "sharpe = calculate_sharpe(results_df)\n",
        "sortino = calculate_sortino(results_df)\n",
        "mdd = calculate_mdd(results_df)\n",
        "\n",
        "print(f\"CAGR: {cagr:.2%}\")\n",
        "print(f\"Sharpe Ratio: {sharpe:.2f}\")\n",
        "print(f\"Sortino Ratio: {sortino:.2f}\")\n",
        "print(f\"Max Drawdown: {mdd:.2%}\")\n",
        "\n",
        "# Save locally\n",
        "results_df.to_csv(\"backtest_results.csv\", index=False)\n",
        "pd.DataFrame(trade_signals).to_csv(\"trade_signals.csv\", index=False)\n",
        "pd.DataFrame({\"CAGR\": [cagr], \"Sharpe\": [sharpe], \"Sortino\": [sortino], \"Max_Drawdown\": [mdd]}).to_csv(\"backtest_summary.csv\", index=False)\n",
        "\n",
        "# Upload to Blob\n",
        "upload_to_blob(\"backtest_results.csv\", f\"{OUTPUT_FOLDER}/backtest_results.csv\")\n",
        "upload_to_blob(\"trade_signals.csv\", f\"{OUTPUT_FOLDER}/trade_signals.csv\")\n",
        "upload_to_blob(\"backtest_summary.csv\", f\"{OUTPUT_FOLDER}/backtest_summary.csv\")\n",
        "\n",
        "print(\"âœ… Backtest complete & uploaded to Blob.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c34c92-95cc-4fe9-b631-b68b798cc749",
      "metadata": {
        "id": "41c34c92-95cc-4fe9-b631-b68b798cc749"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa20a51-9e9a-4b30-ac95-42ae6cad3180",
      "metadata": {
        "id": "0aa20a51-9e9a-4b30-ac95-42ae6cad3180"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d1a7ba-1d54-4f68-9020-6f54eb8d074c",
      "metadata": {
        "id": "c5d1a7ba-1d54-4f68-9020-6f54eb8d074c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1a50d7b-22b0-45a6-888a-8f99e8ab6447",
      "metadata": {
        "id": "f1a50d7b-22b0-45a6-888a-8f99e8ab6447"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd2fcd34-b4f8-413f-a754-224d266c02d0",
      "metadata": {
        "id": "dd2fcd34-b4f8-413f-a754-224d266c02d0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4164dcce-1aad-4e82-94f6-039eee2e70fe",
      "metadata": {
        "id": "4164dcce-1aad-4e82-94f6-039eee2e70fe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}